{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the data\n",
    "%   \n",
    "%   Use this code to get a data set (array of RTs from a single condition) \n",
    "%   to fit, already preprocessed to include correct trials only and remove\n",
    "%   outliers (including express saccades). See later_getData for details\n",
    "data = later_getData([], [], 0.2);\n",
    "RTs = data{1};\n",
    "clear data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From GetData tab: Each raw data file in data/data_mgl/F has the following vectors (in \n",
    "%    each case, columns are individual trials):\n",
    "%     - decisionSum takes -1 if the decision was left side, and 1 if the \n",
    "%        decision was right side.\n",
    "%     - labelSum takes 1 for trials after change point (TACP) 0, and 2 \n",
    "%        for TACP 1, and 3 for TACP 2, and 4 for TACP 3, and 5 for TACP 4, \n",
    "%        and 0 for the rest. [NOTE FROM JIG: THIS IS HOW MY STUDENT TIM KIM\n",
    "%        CODED THE DATA, SO I WANT TO KEEP IT IN THIS RAW FORMAT. HOWEVER, \n",
    "%        PLEASE NOTE THAT THIS CODING SCHEME SEEMS OVERLY CONFUSING; I\n",
    "%        WOULD HAVE CODED IT AS 0 FOR TACP=0, 1 FOR TACP=1, ETC]\n",
    "%     - numdirSum takes -1 if the sound was left side, and 1 if the sound \n",
    "%        was right side.\n",
    "%     - percorrSum is 0 if the subject's answer was incorrect, and 1 \n",
    "%        if the subject's answer was correct.\n",
    "%     - syncSum is 1 if the current trial is a \"pupil trial\" and 0 if \n",
    "%        the current trial is \"RT trial\" [NOTE FROM JIG: IGNORED HERE]\n",
    "%     - tRxnSum is RT measured by mglGetSecs, where the RT is defined \n",
    "%        as the time when the eyes leave the fixation window. \n",
    "%        The fixation window was defined as 30% of the height and width of \n",
    "%        the screen (32.31cm x 51.69cm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def later_get_data(subject_tag=None, data_directory=None, express_cutoff=None):\n",
    "    \"\"\"\n",
    "    Retrieve experimental data and associated labels based on the given parameters.\n",
    "\n",
    "    Parameters:\n",
    "    subject_tag : str or None\n",
    "        Identifier for the subject (default is None).\n",
    "    data_directory : str or None\n",
    "        Path to the directory containing data files (default is None).\n",
    "    express_cutoff : float\n",
    "        Threshold value used to filter or categorize data (default is 0.2).\n",
    "\n",
    "    Returns:\n",
    "    tuple\n",
    "        A tuple containing two elements:\n",
    "        - data_ : list\n",
    "            The main data output, such as response times or experimental data.\n",
    "        - labels_ : list\n",
    "            Associated labels or metadata related to the data.\n",
    "    \"\"\"\n",
    "    # Set default subject_tag if not provided\n",
    "    if subject_tag is None or subject_tag == '':\n",
    "        subject_tag = 'JT'\n",
    "\n",
    "    # Set default data_directory if not provided\n",
    "    if data_directory is None or data_directory == '':\n",
    "        data_directory = os.path.join('/Users/urodas/Library/Mobile Documents/com~apple~CloudDocs/2024 Fall Semester/Quantitative Neuroscience/LATER Model Fitting Data')\n",
    "\n",
    "    # Set default express_cutoff if not provided\n",
    "    if express_cutoff is None:\n",
    "        express_cutoff = 0.0\n",
    "\n",
    "    # Load the data from the specified subject\n",
    "    mat_file_path = os.path.join(data_directory, 'data_mgl', 'F', f'{subject_tag}_RT.mat')\n",
    "    \n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(mat_file_path)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Data file not found: {mat_file_path}\")\n",
    "\n",
    "    # Extract data and labels from the loaded .mat file\n",
    "    # Adjust these based on the actual structure of your .mat file\n",
    "    data_ = mat_data.get('data_variable_name')  # Replace with the actual variable name in the .mat file\n",
    "    labels_ = mat_data.get('labels_variable_name')  # Replace with the actual variable name in the .mat file\n",
    "    \n",
    "    return data_, labels_\n",
    "\n",
    "\n",
    "\n",
    "def define_selection_criteria(percorr_sum, t_rxn_sum, express_cutoff):\n",
    "    \"\"\"\n",
    "    Define selection criteria for trials based on correctness and reaction times.\n",
    "\n",
    "    Parameters:\n",
    "    percorr_sum : np.ndarray\n",
    "        An array indicating the correctness of trials (1 for correct, 0 for incorrect).\n",
    "    t_rxn_sum : np.ndarray\n",
    "        An array of reaction times for the trials.\n",
    "    express_cutoff : list or np.ndarray\n",
    "        A list or array containing the express cutoff values.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray\n",
    "        A boolean array indicating which trials meet the selection criteria.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure express_cutoff is an array-like object for indexing\n",
    "    express_cutoff = np.array(express_cutoff)\n",
    "\n",
    "    # Define selection criteria\n",
    "    L_trials = (percorr_sum == 1) & (t_rxn_sum > express_cutoff[0]) & (t_rxn_sum < 1.2)\n",
    "\n",
    "    return L_trials\n",
    "\n",
    "\n",
    "def get_data_sets(t_rxn_sum, l_trials, num_dir_sum, label_sum, return_labels=False):\n",
    "    \"\"\"\n",
    "    Collect four datasets based on reaction times and specified conditions.\n",
    "\n",
    "    Parameters:\n",
    "    t_rxn_sum : np.ndarray\n",
    "        Array of reaction times for the trials.\n",
    "    l_trials : np.ndarray\n",
    "        Boolean array indicating which trials meet the selection criteria.\n",
    "    num_dir_sum : np.ndarray\n",
    "        Array indicating the direction of choices (-1 for left, 1 for right).\n",
    "    label_sum : np.ndarray\n",
    "        Array indicating the labels associated with the trials.\n",
    "    return_labels : bool\n",
    "        Whether to return the corresponding labels for the datasets.\n",
    "\n",
    "    Returns:\n",
    "    list\n",
    "        A list containing four datasets corresponding to the specified conditions.\n",
    "    list (optional)\n",
    "        A list of labels if return_labels is True.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the data list\n",
    "    data_ = []\n",
    "    \n",
    "    # C_L,0: Left choices, change-point trials\n",
    "    data_.append(t_rxn_sum[l_trials & (num_dir_sum == -1) & (label_sum == 1)])\n",
    "    \n",
    "    # C_L,1+: Left choices, non-change-point trials\n",
    "    data_.append(t_rxn_sum[l_trials & (num_dir_sum == -1) & (label_sum != 1)])\n",
    "    \n",
    "    # C_R,0: Right choices, change-point trials\n",
    "    data_.append(t_rxn_sum[l_trials & (num_dir_sum == 1) & (label_sum == 1)])\n",
    "    \n",
    "    # C_R,1+: Right choices, non-change-point trials\n",
    "    data_.append(t_rxn_sum[l_trials & (num_dir_sum == 1) & (label_sum != 1)])\n",
    "    \n",
    "    # Return labels if requested\n",
    "    if return_labels:\n",
    "        labels_ = ['Left Choice, No CP', 'Left Choice, CP', 'Right Choice, No CP', 'Right Choice, CP']\n",
    "        return data_, labels_\n",
    "    \n",
    "    return data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the objective function\n",
    "%\n",
    "% The objective function typically defines the error that you want to \n",
    "% minimize between your data and the model predictions. A common objective \n",
    "% function is the negative of the sum of the log-likelihoods of the data, \n",
    "% given the model parameters. To unpack that for the LATER model:\n",
    "%\n",
    "%   1. For each data point (RT from a single trial, in this case) and given\n",
    "%       set of model parameters, compute the probability of the data, given\n",
    "%       the model (i.e., the likelihood)\n",
    "%   2. Take the logarithm\n",
    "%   3. Sum all these log-likelihoods from all the data points\n",
    "%   4. Take the negative, because we want to find the minimum (thus\n",
    "%        corresponding to the maximum likelihood)\n",
    "%\n",
    "%   You can define the function simply using an \"anonymous function\"\n",
    "%   (https://www.mathworks.com/help/matlab/matlab_prog/anonymous-functions.html), \n",
    "%   using this template that assumes that \"fits\" is a 2x1 vector of\n",
    "%   [muR, deltaS]:\n",
    " \n",
    "% EXERCISE:\n",
    "% laterErrFcn = @(fits) <**YOUR OBJECTIVE FUNCTION HERE AS A FUNCTION OF FITS**>;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_negative_log_likelihood(rt_data, model_parameters):\n",
    "    \"\"\"\n",
    "    Compute the negative log-likelihood of the data given the model parameters.\n",
    "\n",
    "    Parameters:\n",
    "    rt_data : np.ndarray\n",
    "        Array of reaction times for the trials.\n",
    "    model_parameters : dict\n",
    "        A dictionary containing the model parameters needed for likelihood calculation.\n",
    "        Example: {'param1': value1, 'param2': value2, ...}\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The negative log-likelihood value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the likelihood array\n",
    "    likelihoods = np.zeros_like(rt_data)\n",
    "\n",
    "    # Example likelihood computation (customize based on your model)\n",
    "    for i, rt in enumerate(rt_data):\n",
    "        # Replace this with the actual likelihood computation based on your model\n",
    "        # For demonstration, assume a simple normal likelihood:\n",
    "        mu = model_parameters.get('mu', 0)  # Mean from model parameters\n",
    "        sigma = model_parameters.get('sigma', 1)  # Standard deviation from model parameters\n",
    "        \n",
    "        # Compute the likelihood for the current RT\n",
    "        likelihoods[i] = (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((rt - mu) / sigma) ** 2)\n",
    "\n",
    "    # Take the logarithm of the likelihoods\n",
    "    log_likelihoods = np.log(likelihoods + 1e-10)  # Adding a small value to avoid log(0)\n",
    "\n",
    "    # Sum all log-likelihoods\n",
    "    total_log_likelihood = np.sum(log_likelihoods)\n",
    "\n",
    "    # Take the negative to get the negative log-likelihood\n",
    "    negative_log_likelihood = -total_log_likelihood\n",
    "\n",
    "    return negative_log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define initial conditions\n",
    "%   \n",
    "%   For the actual fitting, we will use fmincon\n",
    "%   (https://www.mathworks.com/help/optim/ug/fmincon.html), which is \n",
    "%   \"function minimization with constraints.\" This function allows for \n",
    "%   constraints that include upper and lower bounds on the parameters.\n",
    "%   So here we define those bounds, along with the initial values.\n",
    "%   We'll use fairly arbitrary values for the lower and upper\n",
    "%   bounds, but we should pick the initial values more judiciously. HINT: \n",
    "%   Recall that the muR and deltaS should be strongly related to \n",
    "%   empirical summary statistics of `the (reciprocal) RT distribution.\n",
    "lowerBounds = [0.001 0.001];\n",
    "upperBounds = [1000 1000]; \n",
    "\n",
    "% EXERCISE:\n",
    "% initialValues = [<**ADD INITIAL VALUES HERE**>];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameters: mu = 1.566666661603492, sigma = 0.32998317154955026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:316: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  self.H.update(self.x - self.x_prev, self.g - self.g_prev)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define your negative log-likelihood function\n",
    "def compute_negative_log_likelihood(params, rt_data):\n",
    "    mu, sigma = params\n",
    "    likelihoods = (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((rt_data - mu) / sigma) ** 2)\n",
    "    log_likelihoods = np.log(likelihoods + 1e-10)  # Avoid log(0)\n",
    "    return -np.sum(log_likelihoods)  # Return negative log-likelihood\n",
    "\n",
    "# Example reaction time data because I cannot figure out how to extract the data from the given file.\n",
    "rt_data = np.array([1.5, 2.0, 1.2])\n",
    "\n",
    "# Define lower and upper bounds for parameters\n",
    "lower_bounds = [0.001, 0.001]  # mu and sigma\n",
    "upper_bounds = [1000, 1000]\n",
    "\n",
    "# Initial parameter values (you may choose these based on empirical data)\n",
    "initial_params = [1.0, 1.0]  # Initial guesses for mu and sigma\n",
    "\n",
    "# Set up the bounds for the optimization\n",
    "bounds = list(zip(lower_bounds, upper_bounds))\n",
    "\n",
    "# Run the minimization\n",
    "result = minimize(compute_negative_log_likelihood, initial_params, args=(rt_data,),\n",
    "                  bounds=bounds, method='trust-constr')\n",
    "\n",
    "# Check the result\n",
    "if result.success:\n",
    "    optimized_params = result.x\n",
    "    print(f\"Optimized Parameters: mu = {optimized_params[0]}, sigma = {optimized_params[1]}\")\n",
    "else:\n",
    "    print(\"Optimization failed:\", result.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the fits\n",
    "% \n",
    "%   We will be using GlobalSearch . The general advantage of this approach \n",
    "%   is to avoid local minima; for details, see:\n",
    "%   https://www.mathworks.com/help/gads/how-globalsearch-and-multistart-work.html\n",
    "%  \n",
    "%   These options seem to work well, but I don't have a stronger\n",
    "%   rationale for using them. See the Matlab documentation if you really\n",
    "%   want to dive in and understand them, and let me know if you find\n",
    "%   better settings!\n",
    "opts = optimoptions(@fmincon,    ... % \"function minimization with constraints\"\n",
    "   'Algorithm',   'active-set',  ...\n",
    "   'MaxIter',     3000,          ...\n",
    "   'MaxFunEvals', 3000);\n",
    "\n",
    "% Definine the \"optimization problem\" using variables defined above\n",
    "problem = createOptimProblem('fmincon',    ...\n",
    "    'objective',   laterErrFcn,     ... % Use the objective function\n",
    "    'x0',          initialValues,   ... % Initial conditions\n",
    "    'lb',          lowerBounds,     ... % Parameter lower bounds\n",
    "    'ub',          upperBounds,     ... % Parameter upper bounds\n",
    "    'options',     opts);                % Options defined above\n",
    "\n",
    "% Create a GlobalSearch object\n",
    "gs = GlobalSearch;\n",
    "   \n",
    "% Run it, returning the best-fitting parameter values and the negative-\n",
    "% log-likelihood returned by the objective function\n",
    "[fits(ii,:), nllk] = run(gs,problem);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vj/dj1sw4490b357sf04kgc67jw0000gp/T/ipykernel_60626/840910213.py:8: RuntimeWarning: invalid value encountered in log\n",
      "  log_likelihoods = np.log(likelihoods + 1e-10)  # Avoid log(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-fitting Parameters: mu = 1.566666657772318, sigma = 0.3299831585885595\n",
      "Negative Log-Likelihood: 0.9306746721997319\n"
     ]
    }
   ],
   "source": [
    "# Define a function to perform optimization\n",
    "def optimize_with_global_search(rt_data, bounds, n_initial_guesses=10):\n",
    "    best_fit = None\n",
    "    best_nll = float('inf')\n",
    "\n",
    "    for _ in range(n_initial_guesses):\n",
    "        initial_guess = np.random.uniform(low=lower_bounds, high=upper_bounds)\n",
    "        \n",
    "        result = minimize(compute_negative_log_likelihood, initial_guess,\n",
    "                          args=(rt_data,), bounds=bounds, method='trust-constr')\n",
    "\n",
    "        if result.success and result.fun < best_nll:\n",
    "            best_nll = result.fun\n",
    "            best_fit = result.x\n",
    "\n",
    "    return best_fit, best_nll\n",
    "\n",
    "# Run the optimization with a global search approach\n",
    "fits, nllk = optimize_with_global_search(rt_data, bounds)\n",
    "\n",
    "# Print the best-fitting parameters and negative log-likelihood\n",
    "print(f\"Best-fitting Parameters: mu = {fits[0]}, sigma = {fits[1]}\")\n",
    "print(f\"Negative Log-Likelihood: {nllk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate the fits\n",
    "%\n",
    "%   EXERCISE: How do you know if you got a reasonable answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was unable to figure out how to use my script to extract and read the data to complete the exercises. I used a randomly generated numpy aray instead. Thus, I cannot comment on the reasonability of my answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
